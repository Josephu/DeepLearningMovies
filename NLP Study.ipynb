{
 "metadata": {
  "name": "",
  "signature": "sha256:c67e170940e2f3fe907d260efccc0b69993aca92d22239e1017f1af0fbbbdfac"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "train = pd.read_csv(\"./data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
      "train[0:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>id</th>\n",
        "      <th>sentiment</th>\n",
        "      <th>review</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>\"5814_8\"</td>\n",
        "      <td>1</td>\n",
        "      <td>\"With all this stuff going down at the moment ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>\"2381_9\"</td>\n",
        "      <td>1</td>\n",
        "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>\"7759_3\"</td>\n",
        "      <td>0</td>\n",
        "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>\"3630_4\"</td>\n",
        "      <td>0</td>\n",
        "      <td>\"It must be assumed that those who praised thi...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>\"9495_8\"</td>\n",
        "      <td>1</td>\n",
        "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td>\"8196_8\"</td>\n",
        "      <td>1</td>\n",
        "      <td>\"I dont know why people think this is such a b...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td>\"7166_2\"</td>\n",
        "      <td>0</td>\n",
        "      <td>\"This movie could have been very good, but com...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td>\"10633_1\"</td>\n",
        "      <td>0</td>\n",
        "      <td>\"I watched this video at a friend's house. I'm...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td>\"319_1\"</td>\n",
        "      <td>0</td>\n",
        "      <td>\"A friend of mine bought this film for \u00a31, and...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td>\"8713_10\"</td>\n",
        "      <td>1</td>\n",
        "      <td>\"&lt;br /&gt;&lt;br /&gt;This movie is full of references....</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "          id  sentiment                                             review\n",
        "0   \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
        "1   \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
        "2   \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
        "3   \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
        "4   \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ...\n",
        "5   \"8196_8\"          1  \"I dont know why people think this is such a b...\n",
        "6   \"7166_2\"          0  \"This movie could have been very good, but com...\n",
        "7  \"10633_1\"          0  \"I watched this video at a friend's house. I'm...\n",
        "8    \"319_1\"          0  \"A friend of mine bought this film for \u00a31, and...\n",
        "9  \"8713_10\"          1  \"<br /><br />This movie is full of references...."
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print train[\"review\"][0]\n",
      "print train[\"sentiment\"][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n",
        "1\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Cleansing Plan\n",
      "\n",
      "* Remove <br />\n",
      "* Remove '(' and ')'\n",
      "* Remove strange characters, eg. \u00a8\n",
      "* Remove dual quotes eg. '\\\"'\n",
      "* Remove numbers, eg. 2006, 20\n",
      "* Replace all punctuations into space\n",
      "* Remove meaningless words, eg. a, to, in"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import nltk\n",
      "import re\n",
      "\n",
      "def cleanse_review(review):\n",
      "    # Initialize the BeautifulSoup object on a single movie review     \n",
      "    text = BeautifulSoup(review).get_text()\n",
      "\n",
      "    # Use regular expressions to do a find-and-replace\n",
      "    letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
      "                          \" \",                   # The pattern to replace it with\n",
      "                          text )  # The text to search\n",
      "    lower_case = letters_only.lower()        # Convert to lower case\n",
      "    words = lower_case.split()               # Split into words\n",
      "    return words\n",
      "\n",
      "words = cleanse_review(train[\"review\"][0])\n",
      "fdist = nltk.FreqDist(words)\n",
      "fdist.most_common(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "[(u'the', 19),\n",
        " (u'is', 16),\n",
        " (u'this', 11),\n",
        " (u'of', 11),\n",
        " (u'mj', 11),\n",
        " (u'and', 10),\n",
        " (u'a', 10),\n",
        " (u'i', 10),\n",
        " (u'to', 9),\n",
        " (u'he', 9)]"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords # Import the stop word list\n",
      "stop_words_english = set(stopwords.words(\"english\"))\n",
      "words = cleanse_review(train[\"review\"][0])\n",
      "filtered_words = []\n",
      "for word in words:\n",
      "    if not word in stop_words_english:\n",
      "        filtered_words.append(word)\n",
      "print stop_words_english\n",
      "print '---'\n",
      "filtered_fdist = nltk.FreqDist(filtered_words)\n",
      "print filtered_fdist.most_common(20)\n",
      "print '---'\n",
      "morphy_words = [ wn.morphy(w) for w in filtered_words ]\n",
      "filtered_fdist = nltk.FreqDist(morphy_words)\n",
      "print filtered_fdist.most_common(20)\n",
      "print '---'\n",
      "morphy_words = filter(lambda w: w != None, [ wn.morphy(w) for w in filtered_words ])\n",
      "filtered_fdist = nltk.FreqDist(morphy_words)\n",
      "print filtered_fdist.most_common(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set([u'all', u'just', u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'herself', u'had', u'should', u'to', u'only', u'under', u'ours', u'has', u'do', u'them', u'his', u'very', u'they', u'not', u'during', u'now', u'him', u'nor', u'did', u'this', u'she', u'each', u'further', u'where', u'few', u'because', u'doing', u'some', u'are', u'our', u'ourselves', u'out', u'what', u'for', u'while', u'does', u'above', u'between', u't', u'be', u'we', u'who', u'were', u'here', u'hers', u'by', u'on', u'about', u'of', u'against', u's', u'or', u'own', u'into', u'yourself', u'down', u'your', u'from', u'her', u'their', u'there', u'been', u'whom', u'too', u'themselves', u'was', u'until', u'more', u'himself', u'that', u'but', u'don', u'with', u'than', u'those', u'he', u'me', u'myself', u'these', u'up', u'will', u'below', u'can', u'theirs', u'my', u'and', u'then', u'is', u'am', u'it', u'an', u'as', u'itself', u'at', u'have', u'in', u'any', u'if', u'again', u'no', u'when', u'same', u'how', u'other', u'which', u'you', u'after', u'most', u'such', u'why', u'a', u'off', u'i', u'yours', u'so', u'the', u'having', u'once'])\n",
        "---\n",
        "[(u'mj', 11), (u'people', 5), (u'one', 4), (u'going', 3), (u'know', 3), (u'like', 3), (u'bad', 3), (u'sequence', 3), (u'movie', 3), (u'maybe', 3), (u'moonwalker', 2), (u'hate', 2), (u'whole', 2), (u'cool', 2), (u'michael', 2), (u'really', 2), (u'jackson', 2), (u'fans', 2), (u'joe', 2), (u'guy', 2)]\n",
        "---\n",
        "[(None, 28), (u'people', 5), (u'one', 4), (u'hate', 3), (u'going', 3), (u'know', 3), (u'like', 3), (u'bad', 3), (u'sequence', 3), (u'movie', 3), (u'drug', 3), (u'message', 3), (u'want', 3), (u'maybe', 3), (u'whole', 2), (u'watch', 2), (u'fan', 2), (u'cool', 2), (u'michael', 2), (u'guy', 2)]\n",
        "---\n",
        "[(u'people', 5), (u'one', 4), (u'hate', 3), (u'going', 3), (u'know', 3), (u'like', 3), (u'bad', 3), (u'sequence', 3), (u'movie', 3), (u'drug', 3), (u'message', 3), (u'want', 3), (u'maybe', 3), (u'whole', 2), (u'watch', 2), (u'fan', 2), (u'cool', 2), (u'michael', 2), (u'guy', 2), (u'really', 2)]\n"
       ]
      }
     ],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = pd.read_csv('./data/testData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
      "test[0:50]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>id</th>\n",
        "      <th>review</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>\"12311_10\"</td>\n",
        "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>\"8348_2\"</td>\n",
        "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>\"5828_4\"</td>\n",
        "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>\"7186_2\"</td>\n",
        "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>\"12128_7\"</td>\n",
        "      <td>\"A very accurate depiction of small time mob l...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td>\"2913_8\"</td>\n",
        "      <td>\"...as valuable as King Tut's tomb! (OK, maybe...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td>\"4396_1\"</td>\n",
        "      <td>\"This has to be one of the biggest misfires ev...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td>\"395_2\"</td>\n",
        "      <td>\"This is one of those movies I watched, and wo...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td>\"10616_1\"</td>\n",
        "      <td>\"The worst movie i've seen in years (and i've ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td>\"9074_9\"</td>\n",
        "      <td>\"Five medical students (Kevin Bacon, David Lab...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td>\"9252_3\"</td>\n",
        "      <td>\"'The Mill on the Floss' was one of the lesser...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td>\"9896_9\"</td>\n",
        "      <td>\"I just saw this film at the phoenix film fest...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td>\"574_4\"</td>\n",
        "      <td>\"\\\"The Love Letter\\\" is one of those movies th...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td>\"11182_8\"</td>\n",
        "      <td>\"Another fantastic offering from the Monkey Is...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td>\"11656_4\"</td>\n",
        "      <td>\"This was included on the disk \\\"Shorts: Volum...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td>\"2322_4\"</td>\n",
        "      <td>\"I'm not really much of an Abbott &amp; Costello f...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td>\"8703_1\"</td>\n",
        "      <td>\"This movie was dreadful. Biblically very inac...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td>\"7483_1\"</td>\n",
        "      <td>\"I don't think I've ever gave something a 1/10...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td>\"6007_10\"</td>\n",
        "      <td>\"Excellent story-telling and cinematography. P...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td>\"12424_4\"</td>\n",
        "      <td>\"I completely forgot that I'd seen this within...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20</th>\n",
        "      <td>\"4672_1\"</td>\n",
        "      <td>\"I like action movies. I have a softspot for \\...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>21</th>\n",
        "      <td>\"10841_3\"</td>\n",
        "      <td>\"This is one of the worst Sandra Bullock movie...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22</th>\n",
        "      <td>\"8954_7\"</td>\n",
        "      <td>\"Watched this flick on Saturday afternoon cabl...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>23</th>\n",
        "      <td>\"7392_1\"</td>\n",
        "      <td>\"I went to see \\\"TKIA\\\" with high expectations...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>24</th>\n",
        "      <td>\"10288_8\"</td>\n",
        "      <td>\"All credit to writer/director Gilles Mimouni ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25</th>\n",
        "      <td>\"5343_4\"</td>\n",
        "      <td>\"As a writing teacher, there are two ending I ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>26</th>\n",
        "      <td>\"4950_1\"</td>\n",
        "      <td>\"I don't know why this has gotten any decent r...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27</th>\n",
        "      <td>\"9257_4\"</td>\n",
        "      <td>\"This film was released in the UK under the na...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>28</th>\n",
        "      <td>\"8689_3\"</td>\n",
        "      <td>\"Uncle Fred Olen Ray once again gives us a lit...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>29</th>\n",
        "      <td>\"4480_2\"</td>\n",
        "      <td>\"OK, it's watchable if you are sick in bed or ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>30</th>\n",
        "      <td>\"9286_10\"</td>\n",
        "      <td>\"If I was only allowed to watch one program in...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>31</th>\n",
        "      <td>\"12245_10\"</td>\n",
        "      <td>\"A truly terrific, touching film. Female melod...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>32</th>\n",
        "      <td>\"7529_7\"</td>\n",
        "      <td>\"Pepe Le Pew can either really creep you out o...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>33</th>\n",
        "      <td>\"4_4\"</td>\n",
        "      <td>\"Alas, another Costner movie that was an hour ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>34</th>\n",
        "      <td>\"5299_2\"</td>\n",
        "      <td>\"\\\"Get Shorty\\\", \\\"Out of Sight\\\", \\\"Jackie Br...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>35</th>\n",
        "      <td>\"12077_8\"</td>\n",
        "      <td>\"I was still living with my parents when they ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>36</th>\n",
        "      <td>\"10701_2\"</td>\n",
        "      <td>\"This movie stunk. There is not much more to i...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>37</th>\n",
        "      <td>\"11579_1\"</td>\n",
        "      <td>\"****SPOILER ALERT**** My boyfriend, some frie...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>38</th>\n",
        "      <td>\"10993_1\"</td>\n",
        "      <td>\"One of the worst movie I have seen in 2009 so...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>39</th>\n",
        "      <td>\"8913_1\"</td>\n",
        "      <td>\"I love watching steven seagal movies not beca...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>40</th>\n",
        "      <td>\"1350_3\"</td>\n",
        "      <td>\"I saw it tonight and fell asleep in the movie...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>41</th>\n",
        "      <td>\"3804_2\"</td>\n",
        "      <td>\"...but other than that, there's almost no red...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>42</th>\n",
        "      <td>\"5025_3\"</td>\n",
        "      <td>\"Rossini once described rival composer Wagner'...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>43</th>\n",
        "      <td>\"5170_8\"</td>\n",
        "      <td>\"Just watched it on the Hallmark Channel. I wa...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>44</th>\n",
        "      <td>\"8225_8\"</td>\n",
        "      <td>\"Ronald Coleman had been a star of the screen ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>45</th>\n",
        "      <td>\"6427_7\"</td>\n",
        "      <td>\"Footprints is a very interesting movie that i...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>46</th>\n",
        "      <td>\"3165_3\"</td>\n",
        "      <td>\"This love story between an American journalis...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>47</th>\n",
        "      <td>\"7902_9\"</td>\n",
        "      <td>\"This is not your typical Indian film. There i...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>48</th>\n",
        "      <td>\"8208_3\"</td>\n",
        "      <td>\"This movie tries to be more than it is. First...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>49</th>\n",
        "      <td>\"3577_8\"</td>\n",
        "      <td>\"\\\"The bad dreams always come back again like ...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "            id                                             review\n",
        "0   \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
        "1     \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
        "2     \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
        "3     \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
        "4    \"12128_7\"  \"A very accurate depiction of small time mob l...\n",
        "5     \"2913_8\"  \"...as valuable as King Tut's tomb! (OK, maybe...\n",
        "6     \"4396_1\"  \"This has to be one of the biggest misfires ev...\n",
        "7      \"395_2\"  \"This is one of those movies I watched, and wo...\n",
        "8    \"10616_1\"  \"The worst movie i've seen in years (and i've ...\n",
        "9     \"9074_9\"  \"Five medical students (Kevin Bacon, David Lab...\n",
        "10    \"9252_3\"  \"'The Mill on the Floss' was one of the lesser...\n",
        "11    \"9896_9\"  \"I just saw this film at the phoenix film fest...\n",
        "12     \"574_4\"  \"\\\"The Love Letter\\\" is one of those movies th...\n",
        "13   \"11182_8\"  \"Another fantastic offering from the Monkey Is...\n",
        "14   \"11656_4\"  \"This was included on the disk \\\"Shorts: Volum...\n",
        "15    \"2322_4\"  \"I'm not really much of an Abbott & Costello f...\n",
        "16    \"8703_1\"  \"This movie was dreadful. Biblically very inac...\n",
        "17    \"7483_1\"  \"I don't think I've ever gave something a 1/10...\n",
        "18   \"6007_10\"  \"Excellent story-telling and cinematography. P...\n",
        "19   \"12424_4\"  \"I completely forgot that I'd seen this within...\n",
        "20    \"4672_1\"  \"I like action movies. I have a softspot for \\...\n",
        "21   \"10841_3\"  \"This is one of the worst Sandra Bullock movie...\n",
        "22    \"8954_7\"  \"Watched this flick on Saturday afternoon cabl...\n",
        "23    \"7392_1\"  \"I went to see \\\"TKIA\\\" with high expectations...\n",
        "24   \"10288_8\"  \"All credit to writer/director Gilles Mimouni ...\n",
        "25    \"5343_4\"  \"As a writing teacher, there are two ending I ...\n",
        "26    \"4950_1\"  \"I don't know why this has gotten any decent r...\n",
        "27    \"9257_4\"  \"This film was released in the UK under the na...\n",
        "28    \"8689_3\"  \"Uncle Fred Olen Ray once again gives us a lit...\n",
        "29    \"4480_2\"  \"OK, it's watchable if you are sick in bed or ...\n",
        "30   \"9286_10\"  \"If I was only allowed to watch one program in...\n",
        "31  \"12245_10\"  \"A truly terrific, touching film. Female melod...\n",
        "32    \"7529_7\"  \"Pepe Le Pew can either really creep you out o...\n",
        "33       \"4_4\"  \"Alas, another Costner movie that was an hour ...\n",
        "34    \"5299_2\"  \"\\\"Get Shorty\\\", \\\"Out of Sight\\\", \\\"Jackie Br...\n",
        "35   \"12077_8\"  \"I was still living with my parents when they ...\n",
        "36   \"10701_2\"  \"This movie stunk. There is not much more to i...\n",
        "37   \"11579_1\"  \"****SPOILER ALERT**** My boyfriend, some frie...\n",
        "38   \"10993_1\"  \"One of the worst movie I have seen in 2009 so...\n",
        "39    \"8913_1\"  \"I love watching steven seagal movies not beca...\n",
        "40    \"1350_3\"  \"I saw it tonight and fell asleep in the movie...\n",
        "41    \"3804_2\"  \"...but other than that, there's almost no red...\n",
        "42    \"5025_3\"  \"Rossini once described rival composer Wagner'...\n",
        "43    \"5170_8\"  \"Just watched it on the Hallmark Channel. I wa...\n",
        "44    \"8225_8\"  \"Ronald Coleman had been a star of the screen ...\n",
        "45    \"6427_7\"  \"Footprints is a very interesting movie that i...\n",
        "46    \"3165_3\"  \"This love story between an American journalis...\n",
        "47    \"7902_9\"  \"This is not your typical Indian film. There i...\n",
        "48    \"8208_3\"  \"This movie tries to be more than it is. First...\n",
        "49    \"3577_8\"  \"\\\"The bad dreams always come back again like ..."
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result = pd.read_csv('./data/Bag_of_Words_model.csv', header=0)\n",
      "gb = result.groupby('succeed')\n",
      "print gb['id'].agg(['count'])\n",
      "print round(len(result[result['succeed'] == True])/ float(len(result)), 4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "         count\n",
        "succeed       \n",
        "False     1692\n",
        "True      8321\n",
        "0.831\n"
       ]
      }
     ],
     "prompt_number": 159
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# synonyms\n",
      "from nltk.corpus import wordnet as wn\n",
      "syn_sets = wn.synsets('shit')\n",
      "print syn_sets\n",
      "syn_sets = wn.synsets('crap')\n",
      "print syn_sets\n",
      "syn_sets = wn.synsets('bullshit')\n",
      "print syn_sets\n",
      "syn_sets = wn.synsets('asshole')\n",
      "print syn_sets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[Synset('crap.n.01'), Synset('bullshit.n.01'), Synset('jack.n.01'), Synset('shit.n.04'), Synset('asshole.n.01'), Synset('damn.n.01'), Synset('denounce.v.04'), Synset('stool.v.04')]\n",
        "[Synset('crap.n.01'), Synset('bullshit.n.01'), Synset('stool.v.04')]\n",
        "[Synset('bullshit.n.01'), Synset('talk_through_one's_hat.v.01')]\n",
        "[Synset('asshole.n.01'), Synset('arse.n.02')]\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "# Read data from files \n",
      "train = pd.read_csv(\"data/labeledTrainData.tsv\", header=0, \n",
      " delimiter=\"\\t\", quoting=3 )\n",
      "test = pd.read_csv(\"data/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
      "unlabeled_train = pd.read_csv(\"data/unlabeledTrainData.tsv\", header=0, \n",
      " delimiter=\"\\t\", quoting=3 )\n",
      "\n",
      "# Verify the number of reviews that were read (100,000 in total)\n",
      "print \"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
      " \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
      " test[\"review\"].size, unlabeled_train[\"review\"].size )\n",
      "\n",
      "# Import various modules for string cleaning\n",
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def review_to_wordlist( review, remove_stopwords=False ):\n",
      "    # Function to convert a document to a sequence of words,\n",
      "    # optionally removing stop words.  Returns a list of words.\n",
      "    #\n",
      "    # 1. Remove HTML\n",
      "    review_text = BeautifulSoup(review).get_text()\n",
      "    #  \n",
      "    # 2. Remove non-letters\n",
      "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
      "    #\n",
      "    # 3. Convert words to lower case and split them\n",
      "    words = review_text.lower().split()\n",
      "    #\n",
      "    # 4. Optionally remove stop words (false by default)\n",
      "    if remove_stopwords:\n",
      "        stops = set(stopwords.words(\"english\"))\n",
      "        words = [w for w in words if not w in stops]\n",
      "    #\n",
      "    # 5. Return a list of words\n",
      "    return(words)\n",
      "\n",
      "\n",
      "# Download the punkt tokenizer for sentence splitting\n",
      "import nltk.data\n",
      "\n",
      "# Load the punkt tokenizer\n",
      "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "# Define a function to split a review into parsed sentences\n",
      "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
      "    # Function to split a review into parsed sentences. Returns a \n",
      "    # list of sentences, where each sentence is a list of words\n",
      "    #\n",
      "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
      "    raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
      "    #\n",
      "    # 2. Loop over each sentence\n",
      "    sentences = []\n",
      "    for raw_sentence in raw_sentences:\n",
      "        # If a sentence is empty, skip it\n",
      "        if len(raw_sentence) > 0:\n",
      "            # Otherwise, call review_to_wordlist to get a list of words\n",
      "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
      "              remove_stopwords ))\n",
      "    #\n",
      "    # Return the list of sentences (each sentence is a list of words,\n",
      "    # so this returns a list of lists\n",
      "    return sentences\n",
      "\n",
      "sentences = []  # Initialize an empty list of sentences\n",
      "\n",
      "print \"Parsing sentences from training set\"\n",
      "for review in train[\"review\"]:\n",
      "    sentences += review_to_sentences(review, tokenizer)\n",
      "\n",
      "print \"Parsing sentences from unlabeled set\"\n",
      "for review in unlabeled_train[\"review\"]:\n",
      "    sentences += review_to_sentences(review, tokenizer)\n",
      "\n",
      "# Import the built-in logging module and configure it so that Word2Vec \n",
      "# creates nice output messages\n",
      "import logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "\n",
      "# Set values for various parameters\n",
      "num_features = 300    # Word vector dimensionality                      \n",
      "min_word_count = 40   # Minimum word count                        \n",
      "num_workers = 4       # Number of threads to run in parallel\n",
      "context = 10          # Context window size                                                                                    \n",
      "downsampling = 1e-3   # Downsample setting for frequent words\n",
      "\n",
      "# Initialize and train the model (this will take some time)\n",
      "from gensim.models import word2vec\n",
      "print \"Training model...\"\n",
      "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
      "            size=num_features, min_count = min_word_count, \\\n",
      "            window = context, sample = downsampling)\n",
      "\n",
      "# If you don't plan to train the model any further, calling \n",
      "# init_sims will make the model much more memory-efficient.\n",
      "model.init_sims(replace=True)\n",
      "\n",
      "# It can be helpful to create a meaningful model name and \n",
      "# save the model for later use. You can load it later using Word2Vec.load()\n",
      "model_name = \"300features_40minwords_10context\"\n",
      "model.save(model_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
        "\n",
        "Parsing sentences from training set"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Parsing sentences from unlabeled set"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training model..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 139
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " model.most_similar(\"awful\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 155,
       "text": [
        "[(u'terrible', 0.6756383180618286),\n",
        " (u'horrible', 0.6110384464263916),\n",
        " (u'dreadful', 0.5766934156417847),\n",
        " (u'atrocious', 0.5758540034294128),\n",
        " (u'horrendous', 0.5423074960708618),\n",
        " (u'abysmal', 0.5276533365249634),\n",
        " (u'laughable', 0.5188220739364624),\n",
        " (u'embarrassing', 0.5136074423789978),\n",
        " (u'appalling', 0.5072193145751953),\n",
        " (u'amateurish', 0.4908950924873352)]"
       ]
      }
     ],
     "prompt_number": 155
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.syn0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 158,
       "text": [
        "array([[-0.12280106, -0.07448322, -0.06097808, ...,  0.05458201,\n",
        "        -0.09045825,  0.01750572],\n",
        "       [-0.01916868, -0.04193816,  0.10834964, ..., -0.00308958,\n",
        "         0.01170176,  0.0818541 ],\n",
        "       [-0.15207744, -0.02848005,  0.01817423, ...,  0.10547014,\n",
        "         0.00679067,  0.06630729],\n",
        "       ..., \n",
        "       [-0.04052395,  0.02107   ,  0.0131762 , ...,  0.0794499 ,\n",
        "        -0.08113613,  0.15719058],\n",
        "       [-0.01921711, -0.04353738,  0.03909666, ...,  0.0140638 ,\n",
        "        -0.09010683, -0.01565588],\n",
        "       [ 0.01547866,  0.01412588,  0.0556228 , ...,  0.04098408,\n",
        "         0.12397652, -0.10208952]], dtype=float32)"
       ]
      }
     ],
     "prompt_number": 158
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result = pd.read_csv('./data/Bag_of_Words_model_linear.csv', header=0)\n",
      "result\n",
      "gb = result.groupby('succeed')\n",
      "print gb['id'].agg(['count'])\n",
      "print round(len(result[result['succeed'] == True])/ float(len(result)), 4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "         count\n",
        "succeed       \n",
        "False     1173\n",
        "True      8772\n",
        "0.8821\n"
       ]
      }
     ],
     "prompt_number": 162
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}